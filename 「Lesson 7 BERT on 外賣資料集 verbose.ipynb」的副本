{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"「Lesson 7: BERT on 外賣資料集 verbose.ipynb」的副本","provenance":[{"file_id":"https://github.com/UDICatNCHU/PyTorch-Tutorial/blob/master/Lesson_7_BERT_on_%E5%A4%96%E8%B3%A3%E8%B3%87%E6%96%99%E9%9B%86_verbose.ipynb","timestamp":1605330980290}],"collapsed_sections":["kTBPkhn3NwyT","9A0d7j-4WYfk","ZKEBsX-9jVsl","Dp7cT2IeBU8X"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"kTBPkhn3NwyT"},"source":["### **環境設定**"]},{"cell_type":"code","metadata":{"id":"EwDUPX_GAFoa","executionInfo":{"status":"ok","timestamp":1605335560127,"user_tz":-480,"elapsed":852,"user":{"displayName":"Ray Chang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhgouxR1rRviRwb4aMdwHGpBXiSH6c6SjV6jrzroKY=s64","userId":"00092246277447951308"}},"outputId":"3799d603-a7e5-4fcc-8253-685cb0eb32a4","colab":{"base_uri":"https://localhost:8080/"}},"source":["!free -h"],"execution_count":null,"outputs":[{"output_type":"stream","text":["              total        used        free      shared  buff/cache   available\n","Mem:            12G        5.7G        2.7G         10M        4.4G        9.0G\n","Swap:            0B          0B          0B\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3L-rpcqJJ6MQ"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"okpsnijCN5xr"},"source":["### **取得 google drive 存取權限**"]},{"cell_type":"code","metadata":{"id":"qu_53dSWe6vc","executionInfo":{"status":"ok","timestamp":1605335562475,"user_tz":-480,"elapsed":954,"user":{"displayName":"Ray Chang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhgouxR1rRviRwb4aMdwHGpBXiSH6c6SjV6jrzroKY=s64","userId":"00092246277447951308"}},"outputId":"adab138f-2b3b-4fdb-c9be-b9de03e8cbc0","colab":{"base_uri":"https://localhost:8080/"}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ft9Jzub0X0iN","executionInfo":{"status":"ok","timestamp":1605335563929,"user_tz":-480,"elapsed":812,"user":{"displayName":"Ray Chang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhgouxR1rRviRwb4aMdwHGpBXiSH6c6SjV6jrzroKY=s64","userId":"00092246277447951308"}},"outputId":"e0c2345e-6a6f-4926-d713-c09cbd1e3d84","colab":{"base_uri":"https://localhost:8080/"}},"source":["cd /content/drive/My Drive/Colab Notebooks/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9EV0IjAmsloF","executionInfo":{"status":"ok","timestamp":1605335566533,"user_tz":-480,"elapsed":1885,"user":{"displayName":"Ray Chang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhgouxR1rRviRwb4aMdwHGpBXiSH6c6SjV6jrzroKY=s64","userId":"00092246277447951308"}},"outputId":"d2be3016-0153-47cc-897b-73c54657847c","colab":{"base_uri":"https://localhost:8080/"}},"source":["!wget https://www.dropbox.com/s/ayihrt5n8kv4ofx/waimai_10k_zh_tw.csv?dl=0"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2020-11-14 06:32:45--  https://www.dropbox.com/s/ayihrt5n8kv4ofx/waimai_10k_zh_tw.csv?dl=0\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.4.1, 2620:100:601c:1::a27d:601\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.4.1|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: /s/raw/ayihrt5n8kv4ofx/waimai_10k_zh_tw.csv [following]\n","--2020-11-14 06:32:45--  https://www.dropbox.com/s/raw/ayihrt5n8kv4ofx/waimai_10k_zh_tw.csv\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://uc18f35cf47e6f742b8ca9c839fb.dl.dropboxusercontent.com/cd/0/inline/BDKSXzCU1U9iUyqN6K8pUadM07YC6CqpJK5N3bLNVzKAMApnjkqovHAR6q-lYMfHsWXGtOqqg6LsQF93EG4jHD2Zh9GMfWQ2vOcaOW1sH4qxpaPkoWh2nJM8PWjl4oDegrw/file# [following]\n","--2020-11-14 06:32:45--  https://uc18f35cf47e6f742b8ca9c839fb.dl.dropboxusercontent.com/cd/0/inline/BDKSXzCU1U9iUyqN6K8pUadM07YC6CqpJK5N3bLNVzKAMApnjkqovHAR6q-lYMfHsWXGtOqqg6LsQF93EG4jHD2Zh9GMfWQ2vOcaOW1sH4qxpaPkoWh2nJM8PWjl4oDegrw/file\n","Resolving uc18f35cf47e6f742b8ca9c839fb.dl.dropboxusercontent.com (uc18f35cf47e6f742b8ca9c839fb.dl.dropboxusercontent.com)... 162.125.6.15, 2620:100:6030:15::a27d:500f\n","Connecting to uc18f35cf47e6f742b8ca9c839fb.dl.dropboxusercontent.com (uc18f35cf47e6f742b8ca9c839fb.dl.dropboxusercontent.com)|162.125.6.15|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 919380 (898K) [text/plain]\n","Saving to: ‘waimai_10k_zh_tw.csv?dl=0’\n","\n","waimai_10k_zh_tw.cs 100%[===================>] 897.83K  --.-KB/s    in 0.01s   \n","\n","2020-11-14 06:32:46 (64.4 MB/s) - ‘waimai_10k_zh_tw.csv?dl=0’ saved [919380/919380]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eo8JiAvSr8l-"},"source":["!mv 'waimai_10k_zh_tw.csv?dl=0' waimai_10k_zh_tw.csv"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8G-s_DPItZnY","executionInfo":{"status":"ok","timestamp":1605335573491,"user_tz":-480,"elapsed":1066,"user":{"displayName":"Ray Chang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhgouxR1rRviRwb4aMdwHGpBXiSH6c6SjV6jrzroKY=s64","userId":"00092246277447951308"}},"outputId":"6d9f9ebb-1d6a-4d29-89f8-208e12034da7","colab":{"base_uri":"https://localhost:8080/"}},"source":["!head waimai_10k_zh_tw.csv"],"execution_count":null,"outputs":[{"output_type":"stream","text":["label,review\n","1,很快，好吃，味道足，量大\n","1,沒有送水沒有送水沒有送水\n","1,非常快，態度好。\n","1,方便，快捷，味道可口，快遞給力\n","1,菜味道很棒！送餐很及時！\n","1,今天師傅是不是手抖了，微辣格外辣！\n","1,\"送餐快,態度也特別好,辛苦啦謝謝\"\n","1,超級快就送到了，這麼冷的天氣騎士們辛苦了。謝謝你們。麻辣香鍋依然很好吃。\n","1,經過上次晚了2小時，這次超級快，20分鐘就送到了……\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"__EAXsDQO8VA","executionInfo":{"status":"ok","timestamp":1605335577029,"user_tz":-480,"elapsed":1193,"user":{"displayName":"Ray Chang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhgouxR1rRviRwb4aMdwHGpBXiSH6c6SjV6jrzroKY=s64","userId":"00092246277447951308"}},"outputId":"b718c023-5a1b-4a3f-c450-66e2866b54f2","colab":{"base_uri":"https://localhost:8080/"}},"source":["!wc waimai_10k_zh_tw.csv"],"execution_count":null,"outputs":[{"output_type":"stream","text":[" 11988  11988 919380 waimai_10k_zh_tw.csv\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9A0d7j-4WYfk"},"source":["### **切 training data 及 testing data**\n"]},{"cell_type":"code","metadata":{"id":"S5q_BxSKO7hO"},"source":["# 將要訓練的句子存起來\n","def SaveSentence(filepath, sent_list):\n","    f = open(filepath, 'w', encoding='UTF-8')\n","    for sent in sent_list:\n","        f.write(sent + '\\n')\n","    f.close()\n","\n","# 分割train_data和test_data\n","def data_Split(FileName):\n","\n","    fp = open(FileName, 'r', encoding='utf-8')\n","    line = fp.readline()        # 第一行是label,review\n","    line = fp.readline()\n","\n","    train_sent_num = 3000     # 實際training data是這個數字的兩倍\n","    test_sent_num = 1000\n","    # 計算目前資料筆數\n","    train_positive_num = 0 \n","    train_negative_num = 0\n","    test_positive_num = 0\n","    test_negative_num = 0\n","    train_data = []\n","    test_data = []\n","\n","    # 用 while 逐行讀取檔案內容，直至檔案結尾\n","    while line:\n","        sent = ''\n","        sent = line.replace('\\n', '')\n","\n","        if line[:2] == '1,':\n","            if train_positive_num < train_sent_num:\n","                train_data.append(sent)\n","                train_positive_num += 1\n","            elif test_positive_num < test_sent_num:\n","                test_data.append(sent)\n","                test_positive_num += 1\n","        else:\n","            if train_negative_num < train_sent_num:\n","                train_data.append(sent)\n","                train_negative_num += 1\n","            elif test_negative_num < test_sent_num:\n","                test_data.append(sent)\n","                test_negative_num += 1\n","        \n","        line = fp.readline()\n","    \n","    fp.close()\n","\n","    SaveSentence('train_data.txt', train_data)\n","    SaveSentence('test_data.txt', test_data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PFWR9XNPWs-9"},"source":["data_Split('waimai_10k_zh_tw.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kC-zNaj_t3bN","executionInfo":{"status":"ok","timestamp":1605335579734,"user_tz":-480,"elapsed":1152,"user":{"displayName":"Ray Chang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhgouxR1rRviRwb4aMdwHGpBXiSH6c6SjV6jrzroKY=s64","userId":"00092246277447951308"}},"outputId":"5c16a9cb-7577-46c7-dabf-6044e11b2523","colab":{"base_uri":"https://localhost:8080/"}},"source":["!wc train_data.txt"],"execution_count":null,"outputs":[{"output_type":"stream","text":["  6000   6000 430529 train_data.txt\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZKEBsX-9jVsl"},"source":["### **安裝所需的函式庫**"]},{"cell_type":"code","metadata":{"id":"pQNrfvGA1Tdb","executionInfo":{"status":"ok","timestamp":1605335585120,"user_tz":-480,"elapsed":3233,"user":{"displayName":"Ray Chang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhgouxR1rRviRwb4aMdwHGpBXiSH6c6SjV6jrzroKY=s64","userId":"00092246277447951308"}},"outputId":"d248a5b8-80a6-4dc0-a190-7948ad076eab","colab":{"base_uri":"https://localhost:8080/"}},"source":["!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.5.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n","Requirement already satisfied: tokenizers==0.9.3 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: sentencepiece==0.1.91 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UM5pihYKZqxy"},"source":["%tensorflow_version 2.x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W-ANDs98mfvE"},"source":["import os\n","import pickle\n","import torch\n","from transformers import BertConfig, BertTokenizer, BertForSequenceClassification, AdamW\n","from torch.utils.data import DataLoader, TensorDataset\n","import torch.nn.functional as F # 激勵函數"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mAsW8-fPvMTW","executionInfo":{"status":"ok","timestamp":1605335586111,"user_tz":-480,"elapsed":4214,"user":{"displayName":"Ray Chang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhgouxR1rRviRwb4aMdwHGpBXiSH6c6SjV6jrzroKY=s64","userId":"00092246277447951308"}},"outputId":"964257d0-801a-4c70-bcbd-4ef097df90da","colab":{"base_uri":"https://localhost:8080/"}},"source":["!wget https://www.dropbox.com/s/fxa234s3de7k55q/bert-base-chinese-vocab.txt?dl=0"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2020-11-14 06:33:04--  https://www.dropbox.com/s/fxa234s3de7k55q/bert-base-chinese-vocab.txt?dl=0\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.6.1, 2620:100:601c:1::a27d:601\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.6.1|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: /s/raw/fxa234s3de7k55q/bert-base-chinese-vocab.txt [following]\n","--2020-11-14 06:33:04--  https://www.dropbox.com/s/raw/fxa234s3de7k55q/bert-base-chinese-vocab.txt\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://uc735886f7bbff63121b3befca82.dl.dropboxusercontent.com/cd/0/inline/BDJUDUt-VGbM5OYg6O9ORcq38kc6JFcOCfpXkPS_9lQsmPoygzdXUTDxSb_insBHDB4afOle8pgvSE4pE1FSwJei-mSXUMHXaYw9JT6yY712ek9xHEJ-Uduv5GfWShZ_AzQ/file# [following]\n","--2020-11-14 06:33:05--  https://uc735886f7bbff63121b3befca82.dl.dropboxusercontent.com/cd/0/inline/BDJUDUt-VGbM5OYg6O9ORcq38kc6JFcOCfpXkPS_9lQsmPoygzdXUTDxSb_insBHDB4afOle8pgvSE4pE1FSwJei-mSXUMHXaYw9JT6yY712ek9xHEJ-Uduv5GfWShZ_AzQ/file\n","Resolving uc735886f7bbff63121b3befca82.dl.dropboxusercontent.com (uc735886f7bbff63121b3befca82.dl.dropboxusercontent.com)... 162.125.6.15, 2620:100:601c:15::a27d:60f\n","Connecting to uc735886f7bbff63121b3befca82.dl.dropboxusercontent.com (uc735886f7bbff63121b3befca82.dl.dropboxusercontent.com)|162.125.6.15|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 109540 (107K) [text/plain]\n","Saving to: ‘bert-base-chinese-vocab.txt?dl=0’\n","\n","bert-base-chinese-v 100%[===================>] 106.97K  --.-KB/s    in 0.003s  \n","\n","2020-11-14 06:33:05 (34.3 MB/s) - ‘bert-base-chinese-vocab.txt?dl=0’ saved [109540/109540]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9qjG9viUvRnE"},"source":["!mv 'bert-base-chinese-vocab.txt?dl=0' bert-base-chinese-vocab.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fhvyxSugvjOj","executionInfo":{"status":"ok","timestamp":1605335586449,"user_tz":-480,"elapsed":4545,"user":{"displayName":"Ray Chang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhgouxR1rRviRwb4aMdwHGpBXiSH6c6SjV6jrzroKY=s64","userId":"00092246277447951308"}},"outputId":"8877066b-031e-4624-c338-0757545421cd","colab":{"base_uri":"https://localhost:8080/"}},"source":["!head bert-base-chinese-vocab.txt"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[PAD]\n","[unused1]\n","[unused2]\n","[unused3]\n","[unused4]\n","[unused5]\n","[unused6]\n","[unused7]\n","[unused8]\n","[unused9]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Dp7cT2IeBU8X"},"source":["### **將資料轉換成輸入格式**"]},{"cell_type":"code","metadata":{"id":"JIxND7_CcgSg"},"source":["def convert_data_to_feature(FileName):\n","    # 載入字典\n","    tokenizer = BertTokenizer(vocab_file='bert-base-chinese-vocab.txt')\n","\n","    # 載入資料\n","    Labels = []\n","    Sentences = []\n","    with open(FileName,'r',encoding='utf-8') as f:\n","        data = f.read()\n","    LS_pairs = data.split(\"\\n\")\n","\n","    for LS_pair in LS_pairs:\n","        if LS_pair != \"\":\n","            try:\n","                L = LS_pair[:1]\n","                S = LS_pair[2:]\n","                Labels.append(int(L))\n","                Sentences.append(S)\n","            except:\n","                continue\n","    \n","    assert len(Labels) == len(Sentences)\n","\n","    # BERT input embedding\n","    max_seq_len = 0     # 紀錄最大長度\n","    input_ids = []\n","    original_length = []    # 紀錄原本長度\n","    for S in Sentences:\n","        # 將句子切割成一個個token\n","        word_piece_list = tokenizer.tokenize(S)\n","        # 將token轉成字典中的id\n","        input_id = tokenizer.convert_tokens_to_ids(word_piece_list)\n","        # 補上[CLS]和[SEP]\n","        input_id = tokenizer.build_inputs_with_special_tokens(input_id)\n","\n","        if(len(input_id)>max_seq_len):\n","            max_seq_len = len(input_id)\n","        input_ids.append(input_id)\n","\n","    print(\"最長句子長度:\",max_seq_len)\n","    assert max_seq_len <= 512 # 小於BERT-base長度限制\n","    max_seq_len = 512\n","\n","    # 補齊長度\n","    for c in input_ids:\n","        # 紀錄原本長度\n","        length = len(c)\n","        original_length.append(length)\n","        while len(c)<max_seq_len:\n","            c.append(0)\n","    \n","    token_type_ids = [[0]*max_seq_len for i in range(len(Sentences))]         # token_type_ids # 儲存的是句子的id，id為0就是第一句，id為1就是第二句\n","    attention_mask = []                                                      # attention_mask # 1代表是真實的單詞id，0代表補齊長度\n","    for i in range(len(Sentences)):\n","        attention_id = []\n","        for j in range(original_length[i]):\n","            attention_id.append(1)\n","        while len(attention_id)<max_seq_len:\n","            attention_id.append(0)\n","        attention_mask.append(attention_id)\n","\n","    assert len(input_ids) == len(token_type_ids) and len(input_ids) == len(attention_mask) and len(input_ids) == len(Labels)\n","\n","    data_features = {'input_ids':input_ids,\n","                    'token_type_ids':token_type_ids,\n","                    'attention_mask':attention_mask,\n","                    'labels':Labels}\n","\n","    return data_features"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ls8HGD6LCOdJ"},"source":["### **輸入格式轉Dataset**"]},{"cell_type":"code","metadata":{"id":"vMZ8L2eKd-Gc"},"source":["def makeDataset(data_feature):\n","    input_ids = data_feature['input_ids']\n","    token_type_ids = data_feature['token_type_ids']\n","    attention_mask = data_feature['attention_mask']\n","    labels = data_feature['labels']\n","\n","    all_input_ids = torch.tensor([input_id for input_id in input_ids], dtype=torch.long)\n","    all_token_type_ids = torch.tensor([token_type_id for token_type_id in token_type_ids], dtype=torch.long)\n","    all_attention_mask_ids = torch.tensor([attention_id for attention_id in attention_mask], dtype=torch.long)\n","    all_labels = torch.tensor([label for label in labels], dtype=torch.long)\n","    dataset = TensorDataset(all_input_ids, all_token_type_ids, all_attention_mask_ids, all_labels)\n","\n","    return dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2kfAZ7fBCYQO","executionInfo":{"status":"ok","timestamp":1605335598235,"user_tz":-480,"elapsed":3358,"user":{"displayName":"Ray Chang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhgouxR1rRviRwb4aMdwHGpBXiSH6c6SjV6jrzroKY=s64","userId":"00092246277447951308"}},"outputId":"fd8b4058-5a46-44ac-faa4-04d5947ed9b9","colab":{"base_uri":"https://localhost:8080/"}},"source":["tokenizer = BertTokenizer(vocab_file='bert-base-chinese-vocab.txt')\n","data_features = convert_data_to_feature('train_data.txt')\n","\n","print(data_features['input_ids'][5998])\n","print(tokenizer.convert_ids_to_tokens(data_features['input_ids'][5998]))\n","print(data_features['token_type_ids'][5998])\n","print(data_features['attention_mask'][5998])\n","print(data_features['labels'][5998])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["最長句子長度: 436\n","[101, 679, 1468, 1962, 1391, 8024, 2571, 6894, 3302, 1243, 679, 7097, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","['[CLS]', '不', '咋', '好', '吃', '，', '快', '遞', '服', '務', '不', '錯', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jV3XNlQUbvBs"},"source":["### **Fine-Tuning**"]},{"cell_type":"code","metadata":{"id":"F5SYWCMYbw7b"},"source":["# 計算正確值\n","def compute_accuracy(y_pred, y_target):\n","    _, y_pred_indices = y_pred.max(dim=1)\n","    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n","    return n_correct / len(y_pred_indices) * 100"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BO8AFnQvdMzs","executionInfo":{"status":"error","timestamp":1605336201407,"user_tz":-480,"elapsed":593160,"user":{"displayName":"Ray Chang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhgouxR1rRviRwb4aMdwHGpBXiSH6c6SjV6jrzroKY=s64","userId":"00092246277447951308"}},"outputId":"91b2bbb2-454a-4969-c4fb-7540456659d9","colab":{"base_uri":"https://localhost:8080/","height":560}},"source":["# set device\n","device = torch.device('cuda')\n","\n","train_data_feature = convert_data_to_feature('train_data.txt')\n","test_data_feature = convert_data_to_feature('test_data.txt')\n","train_dataset = makeDataset(train_data_feature)\n","test_dataset = makeDataset(test_data_feature)\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n","test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=True)\n","\n","model = BertForSequenceClassification.from_pretrained('bert-base-chinese')\n","model.to(device)\n","\n","# Prepare optimizer and schedule (linear warmup and decay)\n","# no_decay = ['bias', 'LayerNorm.weight']\n","# optimizer_grouped_parameters = [\n","#     {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n","#     {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","#     ]\n","Learning_rate = 5e-6       # 學習率\n","optimizer = AdamW(model.parameters(), lr=Learning_rate, eps=1e-8)\n","\n","for epoch in range(1):\n","    # epoch 原本是3，改成1\n","    # 訓練模式\n","    model.train()\n","    All_train_correct = 0.0\n","    AllTrainLoss = 0.0\n","    count = 0\n","    for batch_dict in train_dataloader:\n","        batch_dict = tuple(t.to(device) for t in batch_dict)\n","\n","        outputs = model(\n","            input_ids = batch_dict[0],\n","            token_type_ids = batch_dict[1],             \n","            attention_mask = batch_dict[2],             \n","            labels = batch_dict[3]\n","            )\n","        loss, logits = outputs[:2]\n","            \n","        train_correct = compute_accuracy(logits, batch_dict[3])       # 計算正確率\n","        All_train_correct += train_correct\n","        AllTrainLoss += loss.item()\n","        count += 1\n","\n","        model.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","            \n","    Average_train_correct = round(All_train_correct/count, 3)\n","    Average_train_loss = round(AllTrainLoss/count, 3)\n","\n","    # 測試模式\n","    model.eval()\n","    All_test_correct = 0.0\n","    AllTestLoss = 0.0\n","    count = 0\n","    for batch_dict in test_dataloader:\n","        batch_dict = tuple(t.to(device) for t in batch_dict)\n","\n","        outputs = model(\n","            input_ids = batch_dict[0],\n","            token_type_ids = batch_dict[1],            \n","            attention_mask = batch_dict[2],           \n","            labels = batch_dict[3]\n","            )\n","        loss, logits = outputs[:2]\n","\n","        test_correct = compute_accuracy(logits, batch_dict[3])       # 計算正確率\n","        All_test_correct += test_correct\n","        AllTestLoss += loss.item()\n","\n","        count += 1\n","        \n","    Average_test_correct = round(All_test_correct/count, 3)\n","    Average_test_loss = round(AllTestLoss/count, 3)\n","\n","    print('第' + str(epoch+1) + '次' + '訓練模式，loss為:' + str(Average_train_loss) + ' 正確率為' + str(Average_train_correct)+ '，測試模式，loss為:' + str(Average_test_loss) + ' 正確率為' + str(Average_test_correct))\n","    \n","# 模型存檔\n","model_to_save = model.module if hasattr(model, 'module') else model\n","model_to_save.save_pretrained('trained_model')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["最長句子長度: 436\n","最長句子長度: 307\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-72-38908488dd80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mAverage_train_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAll_train_correct\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/optimization.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    302\u001b[0m                     \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 \u001b[0;31m# Just adding the square of the weights to the loss function is *not*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"P4KOyw5IhyhG"},"source":["### **測試**"]},{"cell_type":"code","metadata":{"id":"AniNPUdGgTUo"},"source":["def to_input_id(sentence_input):\n","    return tokenizer.build_inputs_with_special_tokens(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentence_input)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3uAMmGFch6yc","executionInfo":{"status":"error","timestamp":1605408511656,"user_tz":-480,"elapsed":702,"user":{"displayName":"Ray Chang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhgouxR1rRviRwb4aMdwHGpBXiSH6c6SjV6jrzroKY=s64","userId":"00092246277447951308"}},"outputId":"3ca6faad-846e-4402-dafe-d24a9e76cddb","colab":{"base_uri":"https://localhost:8080/","height":282}},"source":["# load and init\n","tokenizer = BertTokenizer(vocab_file='bert-base-chinese-vocab.txt')\n","config = BertConfig.from_pretrained('trained_model/config.json')\n","#  把設定檔放在google drive裡面的/colab notebooks/trained_model\n","model = BertForSequenceClassification.from_pretrained('trained_model/pytorch_model.bin', from_tf=bool('.ckpt' in 'bert-base-chinese'), config=config)\n","model.eval()\n","\n","print('請輸入句子')\n","sentence = input()\n","\n","input_id = to_input_id(sentence)\n","assert len(input_id) <= 512\n","\n","while len(input_id)<512:\n","    input_id.append(0)\n","\n","input_ids = torch.LongTensor(input_id).unsqueeze(0)\n","\n","# predict時，因為沒有label所以沒有loss\n","outputs = model(input_ids)\n","predicts = outputs[:2]\n","predicts = predicts[0]\n","max_val = torch.max(predicts)\n","predict_label = (predicts == max_val).nonzero().numpy()[0][1] # 在第1維度取最大值並返回索引值 \n","\n","if str(predict_label) == '1':\n","    print('正面')\n","else:\n","    print('負面')"],"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-161e5dfb95b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load and init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bert-base-chinese-vocab.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'trained_model/config.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#  把設定檔放在google drive裡面的/colab notebooks/trained_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'trained_model/pytorch_model.bin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.ckpt'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m'bert-base-chinese'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'BertTokenizer' is not defined"]}]}]}